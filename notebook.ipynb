{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "# test = pd.read_csv('test.csv')\n",
    "# test_labels = pd.read_csv('test_labels.csv')\n",
    "\n",
    "X = train.iloc[:, 0]\n",
    "y = train.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pl = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True)),\n",
    "    ('mnb', OneVsRestClassifier(BernoulliNB()))\n",
    "    ])\n",
    "\n",
    "SVM_pl = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))\n",
    "    ])\n",
    "\n",
    "LR_pl = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "RF_pl = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True)),\n",
    "    ('clf', OneVsRestClassifier(RandomForestClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9062120207919269\n",
      "0.9119533691286069\n",
      "0.910689987128161\n",
      "0.908464809060519\n",
      "0.9078365565129053\n",
      "MEAN: 0.9090313485244238\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores_NB = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx].values, y.iloc[train_idx, :].values\n",
    "    X_test, y_test = X.iloc[test_idx].values, y.iloc[test_idx, :].values\n",
    "\n",
    "    NB_pl.fit(X_train, y_train)\n",
    "    y_pred = NB_pl.predict_proba(X_test)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    scores_NB.append(score)\n",
    "    print(score)\n",
    "    \n",
    "print('MEAN: {}'.format(np.mean(scores_NB)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7564507059714446\n",
      "0.7640329274865851\n",
      "0.7693577129913266\n",
      "0.7549248042584734\n",
      "0.7592589255907579\n",
      "MEAN: 0.7608050152597176\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores_SVC = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx].values, y.iloc[train_idx, :].values\n",
    "    X_test, y_test = X.iloc[test_idx].values, y.iloc[test_idx, :].values\n",
    "    \n",
    "    SVM_pl.fit(X_train, y_train)\n",
    "    y_pred = SVM_pl.predict(X_test)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    scores_SVC.append(score)\n",
    "    print(score)\n",
    "    \n",
    "print('MEAN: {}'.format(np.mean(scores_SVC)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nkf = KFold(n_splits=5, shuffle=True)\\n\\nscores_LR = []\\n\\nfor train_idx, test_idx in kf.split(X):\\n    X_train, y_train = X.iloc[train_idx].values, y.iloc[train_idx, :].values\\n    X_test, y_test = X.iloc[test_idx].values, y.iloc[test_idx, :].values\\n    \\n    LR_pl.fit(X_train, y_train)\\n    y_pred = LR_pl.predict(X_test)\\n    score = roc_auc_score(y_test, y_pred)\\n    scores_LR.append(score)\\n    print(score)\\n    \\nprint('MEAN: {}'.format(np.mean(scores_LR)) )\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores_LR = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx].values, y.iloc[train_idx, :].values\n",
    "    X_test, y_test = X.iloc[test_idx].values, y.iloc[test_idx, :].values\n",
    "    \n",
    "    LR_pl.fit(X_train, y_train)\n",
    "    y_pred = LR_pl.predict(X_test)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    scores_LR.append(score)\n",
    "    print(score)\n",
    "    \n",
    "print('MEAN: {}'.format(np.mean(scores_LR)) )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.601998285330387\n",
      "0.6122517539543583\n",
      "0.6026882544141264\n",
      "0.6085768942693336\n",
      "0.6064119138966888\n",
      "MEAN: 0.6063854203729788\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores_RF = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx].values, y.iloc[train_idx, :].values\n",
    "    X_test, y_test = X.iloc[test_idx].values, y.iloc[test_idx, :].values\n",
    "    \n",
    "    RF_pl.fit(X_train, y_train)\n",
    "    y_pred = RF_pl.predict(X_test)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    scores_RF.append(score)\n",
    "    print(score)\n",
    "    \n",
    "print('MEAN: {}'.format(np.mean(scores_RF)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CUDA devices:  1\n",
      "Quadro K2200\n",
      "<torch.cuda.device object at 0x7fd3fe800320>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, RegexpStemmer\n",
    "from string import punctuation\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print('number of CUDA devices: ',torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.cuda.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = np.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec\n",
    "\n",
    "def make_target(label):\n",
    "    tmp = []\n",
    "    for lab in label:\n",
    "        tmp.append(lab)\n",
    "    return tuple( np.array(tmp) )\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, n_hidden1, n_hidden2):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.hidden1 = nn.Linear(vocab_size, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.out = nn.Linear(n_hidden2, num_labels)\n",
    "\n",
    "    def forward(self, x_val):\n",
    "        x = Variable(x_val, requires_grad=False)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "stop.append(\"!\")\n",
    "stop.append(',')\n",
    "stop.append('')\n",
    "stop.append('=')\n",
    "stop = list(stop)\n",
    "re = RegexpStemmer('[0-9]+')\n",
    "\n",
    " # Preprocess text\n",
    "def preprocess_text(string):\n",
    "    # Annoying things!\n",
    "    string = string.replace(\"=\", \"\")\n",
    "    string = string.replace(\"-\", \"\")\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    tokens = word_tokenize(str(string))\n",
    "    # Punctuations\n",
    "    tokens = [re.stem(token.lower()) for token in tokens if token not in punctuation and token not in stop]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nn = train.comment_text.apply(preprocess_text)\n",
    "Y_nn = train.iloc[:, 1:]\n",
    "\n",
    "# Make naive BOW-presentation\n",
    "word_to_ix = {}\n",
    "for sent in X_nn:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Training-error: 0.336\n",
      "[1/5] ROC-score:  0.831\n",
      "[2/5] Training-error: 0.103\n",
      "[2/5] ROC-score:  0.929\n",
      "[3/5] Training-error: 0.064\n",
      "[3/5] ROC-score:  0.940\n",
      "[4/5] Training-error: 0.050\n",
      "[4/5] ROC-score:  0.950\n",
      "[5/5] Training-error: 0.041\n",
      "[5/5] ROC-score:  0.957\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "HIDDEN1 = 32\n",
    "HIDDEN2 = 16\n",
    "NUM_LABELS = 6\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE, HIDDEN1, HIDDEN2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# MultiLabelSoftMarginLoss - \n",
    "epochs = 5\n",
    "batch_size = 1000\n",
    "num_batches = int(len(X) * 0.9) // batch_size\n",
    "# critize = nn.MultiLabelSoftMarginLoss()\n",
    "critize = nn.BCELoss()\n",
    "\n",
    "X_train = X_nn[:int(len(X_nn) * 0.95)]\n",
    "y_train = Y_nn[:int(len(Y_nn) * 0.95)]\n",
    "\n",
    "X_test = X_nn[int(len(X_nn) * 0.95) + 1 :]\n",
    "y_test = Y_nn[int(len(Y_nn) * 0.95) + 1 :]\n",
    "\n",
    "loss_table = []\n",
    "score_table = []\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(num_batches):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        # Fetch the part of data neededVariable\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        # Prepare data to BOW and make torch vectors\n",
    "        X_batch = np.array([make_bow_vector(comment, word_to_ix) for comment in X_batch])\n",
    "        y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).float(), requires_grad=False)\n",
    "        # GD towards opt\n",
    "        model.zero_grad()\n",
    "        output_fw = model.forward(X_batch)\n",
    "\n",
    "        loss = critize(output_fw, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "\n",
    "    X_batch = X_test\n",
    "    y_batch = y_test\n",
    "    X_batch = np.array([make_bow_vector(comment, word_to_ix) for comment in X_batch])\n",
    "    y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "    X_batch = torch.from_numpy(X_batch).float()\n",
    "    # GD towards opt\n",
    "    model.zero_grad()\n",
    "    output_fw = model.forward(X_batch)\n",
    "    score = roc_auc_score(y_batch, output_fw.data.numpy())\n",
    "\n",
    "    print('[%d/%d] Training-error: %.3f' % (epoch+1, epochs, np.mean(losses)))\n",
    "    print('[%d/%d] ROC-score:  %.3f' % (epoch+1, epochs, score))\n",
    "    \n",
    "    loss_table.append(np.mean(losses))\n",
    "    score_table.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 52s, sys: 39.1 s, total: 15min 31s\n",
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def read_corpus(X):\n",
    "    for i, line in enumerate(X):\n",
    "        line = \" \".join(line)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=55)\n",
    "\n",
    "corpus = list(read_corpus(X_nn))\n",
    "d2v_model.build_vocab(corpus)\n",
    "%time d2v_model.train(corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Training-error: 0.181\n",
      "[1/5] ROC-score:  0.830\n",
      "[2/5] Training-error: 0.108\n",
      "[2/5] ROC-score:  0.858\n",
      "[3/5] Training-error: 0.099\n",
      "[3/5] ROC-score:  0.873\n",
      "[4/5] Training-error: 0.097\n",
      "[4/5] ROC-score:  0.880\n",
      "[5/5] Training-error: 0.095\n",
      "[5/5] ROC-score:  0.885\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 100\n",
    "HIDDEN1 = 256\n",
    "HIDDEN2 = 128\n",
    "NUM_LABELS = 6\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE, HIDDEN1, HIDDEN2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 1000\n",
    "num_batches = int(len(X) * 0.8) // batch_size\n",
    "#critize = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "critize = nn.BCELoss()\n",
    "\n",
    "X_train = X_nn[:int(len(X_nn) * 0.95)]\n",
    "y_train = Y_nn[:int(len(Y_nn) * 0.95)]\n",
    "\n",
    "X_test = X_nn[int(len(X_nn) * 0.95) + 1 :]\n",
    "y_test = Y_nn[int(len(Y_nn) * 0.95) + 1 :]\n",
    "\n",
    "loss_table = []\n",
    "score_table = []\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(num_batches):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        # Fetch the part of data needed\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        # Prepare data to BOW and make torch vectors\n",
    "        X_batch = np.array([d2v_model.infer_vector(comment) for comment in X_batch])\n",
    "        y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "        \n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).float(), requires_grad=False)\n",
    "        # GD towards opt\n",
    "        model.zero_grad()\n",
    "        output_fw = model.forward(X_batch)\n",
    "\n",
    "        loss = critize(output_fw, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "        \n",
    "    X_batch = X_test\n",
    "    y_batch = y_test\n",
    "    X_batch = np.array([d2v_model.infer_vector(comment) for comment in X_batch])\n",
    "    y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "    X_batch = torch.from_numpy(X_batch).float()\n",
    "    # GD towards opt\n",
    "    model.zero_grad()\n",
    "    output_fw = model.forward(X_batch)\n",
    "    score = roc_auc_score(y_batch, output_fw.data.numpy())\n",
    "\n",
    "    print('[%d/%d] Training-error: %.3f' % (epoch+1, epochs, np.mean(losses)))\n",
    "    print('[%d/%d] ROC-score:  %.3f' % (epoch+1, epochs, score))\n",
    "    \n",
    "    loss_table.append(np.mean(losses))\n",
    "    score_table.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
