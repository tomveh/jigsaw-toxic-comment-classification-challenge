\documentclass[a4paper,11pt]{article}
\usepackage{times}
\usepackage[left=2cm,text={17cm, 24cm},top=3cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{upgreek}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{amssymb}
\usepackage[]{algorithm2e}
\usepackage{dcolumn}
\newcolumntype{d}[1]{D{.}{.}{#1}}

\title{Hands-on-NLP project \\ Toxic comment classification challenge}
\author{Jukka Pajunen (597092), Tommi Vehvil√§inen (439613)}
\begin{document}
\maketitle

\section{Introduction }

This report is for the course Hands-on-nlp and it covers implementations for classifying toxic comments. Dataset that was used for solving this problem is available at \ref{datalink}. The problem in question is provided as a Kaggle challenge also available at \ref{datalink}. In the challenge comments have six possible toxic classes that are
\begin{itemize}
\item Toxic comments
\item Severe Toxic comments
\item Obscene comments
\item Threatening comments
\item Insulting comments
\item Identity hate comments
\end{itemize}   
and participants are expected to classify comments to any subset of these classes (so the problem is a multi label classification). Dataset that's provided for the challenge is from Wikipedia's talk page edits and goal of the challenge is to help online discussion to become more productive and respectful. 

In the Kaggle competition, they use evaluation metric of ROC-AUC, that evaluates fraction of true positives against the fraction of false positives. When we look at the leaderboads for the given challenge, we can see that everyone in top 50 have achieved score above $0.98$, which would imply that classification of toxic comments in this context is doable and with simple methods one might be able to achieve decent results. 

For this project we implement a neural network based approach and compare it to sklearn's pre-made optimised approaches. 

\section{Exploratory Data Analysis }

\subsection{Comment division to classes}

When first exploring the data from \ref{datalink}, it can be seen that we have majority of comments that are not classified as harmful (clean) and small number of comments that are classified as harmful (toxic). Table \ref{table:comment_counts} shows how the comments are divided between clean and toxic classes. 

\begin{table}[H]
\centering
\begin{tabular}{| l | c | r |}
\hline
Document class & Count & Percentage \\ \hline
All comments & $159571$ & -  \\ \hline
Clean comments & $143346$ & $89,8\%$ \\ \hline
Toxic comments & $16225$  & $10,2\%$ \\ \hline
\end{tabular}
\caption{Clean comments against toxic comments in training data}
\label{table:comment_counts}
\end{table}

When we look at the toxic comment with more care, table \ref{table:harmfull_counts} shows how the comments are divided between the six classes that were introduced.  

\begin{table}[H]
\centering
\begin{tabular}{| l | c | r |}
\hline
Document class & Count & Percentage \\ \hline
All harmful comments & $16225$ & -  \\ \hline
Toxic comments & $15294$ & $94,3\%$ \\ \hline
Obscene comments & $8449$  & $52,1\%$ \\ \hline
Insulting comments & $7887$  & $48,5\%$ \\ \hline
Identity hate comments & $1405$  & $10,2\%$ \\ \hline
Severe toxic comments & $1595$  & $9,8\%$ \\ \hline
Threatening comments & $478$  & $2,9\%$ \\ \hline
\end{tabular}
\caption{What number of harmful comments belong to each class}
\label{table:harmfull_counts}
\end{table}

Table \ref{table:class_partition} shows how classes $c_1$ and $c2$ co occur as 

\[
\frac{ \text{docs}( c_1 ) \cap \text{docs} ( c_2 ) }{ \text{docs} (c_1) \cup  \text{docs} (c_2) }
\].

\begin{table}[H]
\centering
\begin{tabular}{| l | c | c | c | c | c | r |}
\hline
 & Toxic & Severe toxic & Obscene & Threat & Insult & Identity hate \\
\hline 
Toxic & $100\%$ & $9,4\%$ & $33,4\%$ & $2,8\%$ & $31,7\%$ & $7,8\%$ \\
\hline
Severe toxic & $9,4\%$ & $100\%$  & $15,1\%$ & $5,4\%$ & $14,5\%$ & $10,4\%$  \\
\hline
Obscene & $33,4\%$ & $15,1\%$ & $100\%$ & $3,4\%$ & $3,7\%$ & $10,5\%$  \\
\hline
Threat & $2,8\%$ & $5,4\%$ & $3,4\%$ & $100\%$ & $3,7\%$ & $5,2\%$ \\
\hline
Insult & $31,7\%$ & $14,5\%$ & $37,7\%$ & $3,7\%$ & $100\%$ & $12,5\%$ \\
\hline
Identity hate & $7,8\%$ & $10,4\%$ & $10,5\%$ & $5,2\%$ & $12,5\%$ & $100\%$ \\
\hline
\end{tabular}
\caption{What number of comments classified together versus how they are classified alone}
\label{table:class_partition}
\end{table}

When we explore the data further, we can see that toxic comments are usually classified as multiple classes (e.g. severe toxic comment is always also classified as toxic). Table \ref{table:conditional occurances} shows the conditional co-occurrences.

\begin{table}[H]
\centering
\begin{tabular}{| l  | c | c | c | c | r |}
\hline
 - & Severe toxic & Obscene & Threat & Insult & Identity hate \\ \hline
Toxic & $10,4\%$ & $51,4\%$ & $2,9\%$ & $48\%$ & $1,9\%$ \\ \hline \hline
 - & Toxic & Obscene & Threat & Insult & Identity hate \\ \hline
Severe toxic & $100\%$ & $95,1\%$ & $7\%$ & $86\%$ & $19,6\%$ \\ \hline \hline
 - & Toxic & Severe toxic & Threat & Insult & Identity hate \\ \hline
Obscene & $93,8\%$ & $18\%$ & $3,6\%$ & $72,8\%$ & $12,2\%$ \\ \hline \hline \hline
 - & Toxic & Severe toxic & Obscene & Insult & Identity hate \\ \hline
Threat & $93,9\%$ & $23,4\%$ & $63\%$ & $64,2\%$ & $20,5\%$ \\ \hline \hline \hline
 - & Toxic & Severe toxic & Obscene & Threat & Identity hate \\ \hline
Insult & $93,2\%$ & $17,4\%$ & $78,1\%$ & $3,9\%$ & $14,7\%$ \\ \hline \hline \hline
 - & Toxic & Severe toxic & Obscene & Threat & Insult  \\ \hline
Identity hate & $92,7\%$ & $22,3\%$ & $73,5\%$ & $7\%$ & $82,6\%$ \\ \hline \hline \hline
\end{tabular}
\caption{If comment is classified as a class (left column), how likely it is classified as another class (top row)}
\label{table:conditional occurances}
\end{table}


\subsection{Contents of the comments }

When we look into the comments of each class, we can see in table \ref{table:word_counts} that most of the times clean comments tend to have more words on average. 

\begin{table}[H]
\centering
\begin{tabular}{| l | c | r |}
\hline
class & Words in average in a comment  \\ \hline
clean comments & $42$ \\ \hline
Toxic comments & $35$  \\ \hline
Obscene comments & $35$   \\ \hline
Insulting comments & $34$  \\ \hline
Identity hate comments & $38$ \\ \hline
Severe toxic comments & $63$  \\ \hline
Threatening comments & $42$  \\ \hline
\end{tabular}
\caption{Average amount of words in a comment based on the class}
\label{table:word_counts}
\end{table}

Further diving into the comments, we can find most frequent words for each of the classes, as seen in Table \ref{table:frequent_words}.

\begin{table}[H]
\centering
\begin{tabular}{| l | c | c | c |}
\hline
Class & Most common words & class & Most common words \\ \hline
Clean comments &  I & Toxic comments &  I \\
 &  article & & you \\
 &  the & & fuck \\
 &  page & & the \\
 &  wikipedia & & shit \\ \hline
Severe toxic comments &  Fuck & Obscene comments &  You \\
 &  you & & fuck \\
 &  I  & & I\\
 &  suck & & shit\\
 &  ass & & suck \\ \hline
threatening comments &  I & Insulting comments &  You \\
 &  die & & fuck  \\
 &  you & & I \\
 &  ass & & suck \\
 &  will / kill & & nigger \\ \hline
Identity hate comments &  nigger & & \\
 &  fat & & \\
 &  jew & & \\
 &  I & & \\
 &  you & & \\
\hline
\end{tabular}
\caption{Five most frequent words for each class}
\label{table:frequent_words}
\end{table}

\subsection{Conclusions from the data }

Given the training data, it seems to be dominated by clean comments, which would correspond most likely to real-world situation. 

Looking at the harmful comments, it seems that majority of them are less harmful like toxic or obscene while more serious threatening and hateful comments are much more rare. Also the less harmful comments tend to be classified together often.

Looking at the lengths of the sentences, we can see that toxic comments tend to be a bit shorter, which could be caused by harmful comments be written as a impulse, while non-harmful comments are written with more thought. Exception to this rule is with more severe classes of harmful comments.

When looking at the word-frequencies for the classes, we can see that classes that usually appear together (e.g. severe toxic, obscene), also share common words, while classes that rarely are classified together (e.g. identity hate, threat) tend to have different set of frequent words.

Based on the data, it would seem that we don't need a complex model and a simple bag of words or word vector classifier could be enough. 

\section{Data preprocessing }

\subsection{Example comments}

Looking at the individual comments, we have a lot of special characters and words in different languages. Some of the examples that were contained in the data are shown in table \ref{table:example_commnets}. 

\begin{table}[H]
\centering
\begin{tabular}{| l | c | c |}
\hline
Class & Example comment & observations \\ \hline
Toxic & !!!!11!!"" The adult posters here are immature ....  & Special characters used in \\ & & a way that's not normal in traditional text \\ \hline
Threat & Hi! I am back again! \textbackslash n Last warning! \textbackslash n ... & Line endings \\ \hline
Obscene & How you know he is dead. Its just his plane & Nothing to be preprocessed, \\ & that crashed  & the actual content is toxic \\ \hline
\end{tabular}
\caption{Example comments}
\label{table:example_commnets}
\end{table}

Looking at the observations of the comments, it seems that comments that toxic comments could also sometimes be identified by things that are normally processed out while performing NLP. (e.g. all caps or multiple explanation marks). However, as it can be seen, there are also formally correct toxic comments and therefore using a classifier based on words is a better option.

\subsection{Preprocessing methods}

To do the preprocessing, we then decided to use \textit{nltk}'s word tokenizer while also filtering out punctuations, stopwords and numbers. 

\vline

\begin{algorithm}[H]
 Preproces() \\
 \KwData{Input string S}
 initialize output = '' \\
 S = nltk.tokenize(S)
 \ForAll{words in S} {
	\If{word not in stopwords AND word not in punctuation} {
		output.append(word.lower())	
	} 
 }
 return output
 \caption{Preprocessing of comments}
\end{algorithm}

\pagebreak
\section{Overview of the chosen approaches }

\subsection{Approaches}

As it was observed in the data analysis, a simple method with doc2vec or bag of words could be enough for the task. For the classification, we chose to use methods presented in table \ref{table:methods}. 

\begin{table}[H]
\centering
\begin{tabular}{| l | c |}
\hline
Method & From \\ \hline
Naive Bayes & sklearn \\ \hline
Support Vector Machine & sklearn  \\ \hline
Random Forest & sklearn  \\ \hline
BOW nn classifier & own implementation \\ \hline
DOC2VEC nn classifier & own implementation \\ \hline
DOC2VEC rnn classifier & own implementation \\ \hline
\end{tabular}
\caption{Chosen methods}
\label{table:methods}
\end{table}
Sklearn's methods are used to see how well our implementation does when compared to pre-made methods. 

\vline

\subsection{Own implementations}

\subsection{classifier}

For our first model we used neural network with BOW-vectors. As a input, we take a one hot vector that is connected by 2 hidden layer to a output layer of 6-sigmoids that correspond to the classes. With this implementation we found that it is quite demanding on running, as we have large enough dataset that makes the one hot vector to be rather large. However due to having access to university computer we were able to run the model and ended up with a rather good AUC ROC score, when using $90 \%$ of the data to training and $10 \%$ of the data to testing (figure \ref{fig:bowclass}). 

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{BOW-classifier.png}
\caption{Binary cross entropy loss for training data and ROC AUC score on test data, with epochs}
\label{fig:bowclass}
\end{figure}



\vline

\pagebreak
\subsection{2VEC with gensim}

To overcame the large vocabulary size and to use something learned on the course, we also decided to use D2V model from gensim that would make a input for the neural network. (We experimented on using word to vec, but it didn't produce good results). Doc2Vec model used was trained with gensim using all of the comments in the data. After acquiring the doc2vec model we experimented on passing it to a simple neural network and LSTM-model.

The simple neural network with Doc2Vec used same hidden layers as the BOW-implementation. Figure \ref{fig:d2vclass} has the training loss and ROC AUC scores using same training and testing data as for our classifier. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{D2V-classifier.png}
\caption{Binary cross entropy loss for training data and ROC AUC score on test data, with epochs}
\label{fig:d2vclass}
\end{figure}

Our LSTM had 1 hidden layer connected to a output layer. However we could not get the pytorch implementation to take in more than 1-sentence at a time and results of the neural network were bad. Therefore we did not end up with any reasonable results to display.


\section{Experimental results}

We ran the described models by using the same data between sklearn's methods and same date between neural network-implementations. It was not possible to run a test with neural networks as large as with sklearn due to memory constrains (as our bow-presentation was so large) and therefore we decided to use smaller set of data. Results that were obtained are in the table \ref{table:results}.

\begin{table}[H]
\centering
\begin{tabular}{| l | c | }
\hline
Classifier & ROC AUC score  \\ \hline

Naive Bayes & $90,5$ \\ \hline
Support Vector Machine & $71,9$  \\ \hline
Random Forest & $87,6$  \\ \hline
BOW-classifier & $95,1$ \\ \hline
DOC2VEC classifier & $91,5$ \\ \hline
DOC2VEC rnn classifier & - \\ \hline
\end{tabular}
\caption{Results of the experiments}
\label{table:results}
\end{table}
In the table we can see that our NN-methods made it both past score 90. BOW-presentation performed better out of the two, which might be because we lose some context while doing the dimension mapping.

\section{Conclusions}

As seen on the previous chapter, the sklearns simple methods do well on the problem which might be caused by that the toxic comments have explicit words that are easy to find, even with these simple models. We also were able to use simple neural networks to classify the comments. 

We expect that one could achieve even better results utilizing the capital letter and special symbols in the data that are specific to some inputs and this could be one thing to be improved.

We started to work on the problem too late and spend alot of time having the doc2vec model working, which is why we didn't finish the LSTM-model. Finishing the LSTM would be another thing that should be continued.

\begin{thebibliography}{99}
\bibitem{datalink} Data available at:

\url{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge}.

\end{thebibliography}


%\begin{figure}[h]
%\centering
%\includegraphics[scale=1]{p4.jpg}
%\caption{State transition diagram}
%\end{figure}
\end{document}
