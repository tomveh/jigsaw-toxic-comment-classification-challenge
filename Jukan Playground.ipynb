{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# KINDA EXPERIMENTAL - USED FOR EXPLORATION OF DATA - dont expect this to run as it is\n",
    "# ===========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, RegexpStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer() \n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stop.append(\"!\")\n",
    "stop.append(',')\n",
    "stop.append('')\n",
    "stop.append('=')\n",
    "stop = list(stop)\n",
    "re = RegexpStemmer('[0-9]+')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(string):\n",
    "    # Annoying things!\n",
    "    string = string.replace(\"=\", \"\")\n",
    "    string = string.replace(\"-\", \"\")\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    tokens = word_tokenize(str(string))\n",
    "    # Punctuations\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Stopwords\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    # Numbers\n",
    "    tokens = [re.stem(token) for token in tokens]\n",
    "    # convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return (tokens)\n",
    "\n",
    "# Make unigrams and bygrams from a classified data \n",
    "def make_grams(classified_data):\n",
    "    unigram_frequencies = Counter([])\n",
    "    #bigram_frequencies = Counter([])\n",
    "    \n",
    "    sentances = classified_data[\"comment_text\"]\n",
    "    for sentance in sentances:\n",
    "        unigrams = ngrams(sentance, 1)\n",
    "        # bigrams = ngrams(sentance, 2)\n",
    "        unigram_frequencies += Counter(unigrams)\n",
    "        # bigram_frequencies += Counter(bigrams)\n",
    "    return unigram_frequencies# , bigram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('test.csv', index_col=0)\n",
    "#train_labels = pd.read_csv('test_labels.csv', index_col=0)\n",
    "\n",
    "# Process data!\n",
    "#train[\"comment_text\"] = train.comment_text.apply(preprocess_text)\n",
    "#data = pd.concat([train, train_labels], axis=1)\n",
    "\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "data[\"comment_text\"] = data.comment_text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that gets documents that only belong to only one of the classes\n",
    "def filter_comments(data, toxic = 0, severe_toxic = 0, obscene = 0, threat = 0, insult = 0, identity_hate = 0):\n",
    "    condition = (data.toxic == toxic) & (data.severe_toxic == severe_toxic) & (data.obscene == obscene) & (data.threat == threat) & (data.insult == insult) & (data.identity_hate == identity_hate)\n",
    "    return data[condition]\n",
    "    \n",
    "    \n",
    "toxic_only = filter_comments(data, toxic = 1)\n",
    "severe_toxic_only = filter_comments(data, severe_toxic = 1)\n",
    "obscene_only = filter_comments(data, obscene = 1)\n",
    "threat_only = filter_comments(data, threat = 1)\n",
    "insult_only = filter_comments(data, insult = 1)\n",
    "identity_hate_only = filter_comments(data, identity_hate = 1)\n",
    "cleant_documents = filter_comments(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of documents that belong to ONLY ONE class\n",
      "============\n",
      "Total documents:  159571\n",
      "Clean documents: \t\t0.898321%\t ,count: 143346\n",
      "============\n",
      "Only toxic documents: \t\t0.035508%\t ,count: 5666\n",
      "Only severe toxic documents: \t0.0%\t ,count: 0\n",
      "Only obscene documents: \t0.001987%\t ,count: 317\n",
      "Only threath documents: \t0.000138%\t ,count: 22\n",
      "Only insult documents: \t\t0.001886%\t ,count: 301\n",
      "Only identity documents: \t0.000338%\t ,count: 54\n"
     ]
    }
   ],
   "source": [
    "# Method to print precenage of documents compared to larger subset\n",
    "def partition(message, all_docs, subset_docs):\n",
    "    return message + \"\\t\" +  str(round(len(subset_docs) / len(all_docs), 6)) + \"%\\t ,count: \" + str(len(subset_docs))\n",
    "\n",
    "\n",
    "total_documents = len(data)\n",
    "clean_document_count = len(cleant_documents)\n",
    "toxic_document_count = len(toxic_only)\n",
    "severe_toxic_document_count = len(severe_toxic_only)\n",
    "obscene_document_count = len(obscene_only)\n",
    "threat_document_count = len(threat_only)\n",
    "insult_document_count = len(insult_only)\n",
    "identity_hate_document_count = len(identity_hate_only)\n",
    "\n",
    "print(\"Precentage of documents that belong to ONLY ONE class\")\n",
    "print(\"============\")\n",
    "print(\"Total documents: \", total_documents)\n",
    "print(partition(\"Clean documents: \\t\", data, cleant_documents))\n",
    "print(\"============\")\n",
    "print(partition(\"Only toxic documents: \\t\", data, toxic_only))\n",
    "print(partition(\"Only severe toxic documents: \", data, severe_toxic_only))\n",
    "print(partition(\"Only obscene documents: \", data, obscene_only))\n",
    "print(partition(\"Only threath documents: \", data, threat_only))\n",
    "print(partition(\"Only insult documents: \\t\", data, insult_only))\n",
    "print(partition(\"Only identity documents: \", data, identity_hate_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of documents that belong to a class\n",
      "============\n",
      "Total documents:  159571\n",
      "Clean documents: \t\t0.898321%\t ,count: 143346\n",
      "============\n",
      "Toxic documents: \t\t0.095844%\t ,count: 15294\n",
      "Severe toxic documents: \t0.009996%\t ,count: 1595\n",
      "Obscene documents \t: \t0.052948%\t ,count: 8449\n",
      "Threat documents: \t\t0.002996%\t ,count: 478\n",
      "Insult documents: \t\t0.049364%\t ,count: 7877\n",
      "Identity hate documents: \t0.008805%\t ,count: 1405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10167887648758234"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_toxic = data[(data.toxic == 1)]\n",
    "has_severe_toxic = data[(data.severe_toxic == 1)]\n",
    "has_obscene = data[(data.obscene == 1)]\n",
    "has_threat = data[(data.threat == 1)]\n",
    "has_insult = data[(data.insult == 1)]\n",
    "has_identity_hate = data[(data.identity_hate == 1)]\n",
    "\n",
    "\n",
    "print(\"Precentage of documents that belong to a class\")\n",
    "print(\"============\")\n",
    "print(\"Total documents: \", total_documents)\n",
    "print(partition(\"Clean documents: \\t\", data, cleant_documents))\n",
    "print(\"============\")\n",
    "print(partition(\"Toxic documents: \\t\", data, has_toxic))\n",
    "print(partition(\"Severe toxic documents: \", data, has_severe_toxic))\n",
    "print(partition(\"Obscene documents \\t: \", data, has_obscene))\n",
    "print(partition(\"Threat documents: \\t\", data, has_threat))\n",
    "print(partition(\"Insult documents: \\t\", data, has_insult))\n",
    "print(partition(\"Identity hate documents: \", data, has_identity_hate))\n",
    "\n",
    "( total_documents - clean_document_count ) / total_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[0.943 0.098 0.489 0.028 0.453 0.08 ]\n",
      " [0.098 0.098 0.093 0.007 0.084 0.019]\n",
      " [0.489 0.093 0.521 0.019 0.379 0.064]\n",
      " [0.028 0.007 0.019 0.029 0.019 0.006]\n",
      " [0.453 0.084 0.379 0.019 0.485 0.071]\n",
      " [0.08  0.019 0.064 0.006 0.071 0.087]]\n",
      "\n",
      "\n",
      "[[1.    0.104 0.518 0.029 0.48  0.085]\n",
      " [1.    1.    0.951 0.07  0.86  0.196]\n",
      " [0.938 0.18  1.    0.036 0.728 0.122]\n",
      " [0.939 0.234 0.63  1.    0.642 0.205]\n",
      " [0.932 0.174 0.781 0.039 1.    0.147]\n",
      " [0.927 0.223 0.735 0.07  0.826 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "def filter_comments(data, condition):\n",
    "    return data[condition]\n",
    "\n",
    "classes = {}\n",
    "classes[\"Toxic\"] = (data.toxic == 1)\n",
    "classes[\"SevereToxic\"] = (data.severe_toxic == 1)\n",
    "classes[\"Obscene\"] = (data.obscene == 1)\n",
    "classes[\"Threat\"] = (data.threat == 1)\n",
    "classes[\"Insult\"] = (data.insult == 1)\n",
    "classes[\"IdentityHate\"] = (data.identity_hate == 1)\n",
    "\n",
    "class_library = [\"Toxic\", \"SevereToxic\", \"Obscene\", \"Threat\", \"Insult\", \"IdentityHate\"]\n",
    "freq_matrix = np.zeros((len(classes), len(classes)))\n",
    "some_matrix = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for class1 in class_library:\n",
    "    counter2 = 0\n",
    "    for class2 in class_library:\n",
    "        condition = classes[class1] & classes[class2]\n",
    "        common_elements = filter_comments(data, condition)\n",
    "        freq_matrix[counter1,counter2] =  round(len(common_elements)  / (len(data) - len(cleant_documents)), 3)\n",
    "        counter2 += 1\n",
    "    counter1+= 1\n",
    "        \n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for class1 in class_library:\n",
    "    counter2 = 0\n",
    "    class_elements = filter_comments(data, classes[class1])\n",
    "    for class2 in class_library:\n",
    "        condition2 = classes[class1] & classes[class2]\n",
    "        common_elements = filter_comments(data, condition2)\n",
    "        some_matrix[counter1,counter2] =  round(len(common_elements)  / (len(class_elements)), 3)\n",
    "        counter2 += 1\n",
    "    counter1+= 1\n",
    "    \n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(freq_matrix)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(some_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage ammount words per comment\n",
      "=======\n",
      "Clean docs: \t 42\n",
      "=======\n",
      "Toxic docs: \t 35\n",
      "Severe Toxic: \t 63\n",
      "Obscene docs: \t 35\n",
      "Threat docs: \t 42\n",
      "Insult docs: \t 34\n",
      "Hate docs: \t 38\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many words comments in certain class have\n",
    "def calculate_avarage_words(dataframe):\n",
    "    words = 0\n",
    "    for comment in dataframe[\"comment_text\"]:\n",
    "        words += len(comment)\n",
    "    \n",
    "    return round(words / len(dataframe))\n",
    "\n",
    "\n",
    "print(\"Avarage ammount words per comment\")\n",
    "print(\"=======\")\n",
    "print(\"Clean docs: \\t\", calculate_avarage_words(cleant_documents))\n",
    "print(\"=======\")\n",
    "print(\"Toxic docs: \\t\", calculate_avarage_words(has_toxic))\n",
    "print(\"Severe Toxic: \\t\", calculate_avarage_words(has_severe_toxic))\n",
    "print(\"Obscene docs: \\t\", calculate_avarage_words(has_obscene))\n",
    "print(\"Threat docs: \\t\", calculate_avarage_words(has_threat))\n",
    "print(\"Insult docs: \\t\", calculate_avarage_words(has_insult))\n",
    "print(\"Hate docs: \\t\", calculate_avarage_words(has_identity_hate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\"''\",), 224642),\n",
       " (('i',), 170184),\n",
       " (('``',), 147895),\n",
       " (('',), 83433),\n",
       " (('article',), 53756),\n",
       " (('the',), 43227),\n",
       " (('page',), 43177),\n",
       " (('wikipedia',), 39304),\n",
       " (('talk',), 33660),\n",
       " (('please',), 28601)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_unigram = make_grams(cleant_documents)\n",
    "\n",
    "insult_unigram.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "\n",
    "X = train.iloc[:, 0]\n",
    "Y = train.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "\n",
    "training_data = [ X[0].split() ]\n",
    "word_to_ix = {}\n",
    "for sent in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
