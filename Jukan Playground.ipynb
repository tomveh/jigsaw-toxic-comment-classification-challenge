{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, RegexpStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer() \n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stop.append(\"!\")\n",
    "stop.append(',')\n",
    "stop.append('')\n",
    "stop.append('=')\n",
    "stop = list(stop)\n",
    "re = RegexpStemmer('[0-9]+')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(string):\n",
    "    # Annoying things!\n",
    "    string = string.replace(\"=\", \"\")\n",
    "    string = string.replace(\"-\", \"\")\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    tokens = word_tokenize(str(string))\n",
    "    # Punctuations\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Stopwords\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    # Numbers\n",
    "    tokens = [re.stem(token) for token in tokens]\n",
    "    # convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return (tokens)\n",
    "\n",
    "# Make unigrams and bygrams from a classified data \n",
    "def make_grams(classified_data):\n",
    "    unigram_frequencies = Counter([])\n",
    "    bigram_frequencies = Counter([])\n",
    "    \n",
    "    sentances = classified_data[\"comment_text\"]\n",
    "    for sentance in sentances:\n",
    "        unigrams = ngrams(sentance, 1)\n",
    "        bigrams = ngrams(sentance, 2)\n",
    "        unigram_frequencies += Counter(unigrams)\n",
    "        bigram_frequencies += Counter(bigrams)\n",
    "    return (unigram_frequencies, bigram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntoxic_data = data[(data.toxic == 1)]\\nsevere_toxic_data = data[(data.severe_toxic == 1)]\\nobscene_data = data[(data.obscene == 1)]\\nthreat_data = data[(data.threat == 1)]\\ninsult_data = data[(data.insult == 1)]\\ninsult_data = data[(data.identity_hate == 1)]\\n\\n# Make some grams to inspect the data\\ninsult_unigram, insult_bigram = make_grams(insult_data)\\nthreat_unigram, threat_bigram = make_grams(threat_data)\\nobscene_unigram, obscene_bigram = make_grams(obscene_data)\\ntoxic_unigram, toxic_bigram = make_grams(toxic_data)\\nsevere_toxic_unigram, severe_toxic_bigram = make_grams(severe_toxic_data)\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('test.csv', index_col=0)\n",
    "train_labels = pd.read_csv('test_labels.csv', index_col=0)\n",
    "\n",
    "# Process data!\n",
    "train[\"comment_text\"] = train.comment_text.apply(preprocess_text)\n",
    "data = pd.concat([train, train_labels], axis=1)\n",
    "\n",
    "# Take classified data!\n",
    "\"\"\"\n",
    "toxic_data = data[(data.toxic == 1)]\n",
    "severe_toxic_data = data[(data.severe_toxic == 1)]\n",
    "obscene_data = data[(data.obscene == 1)]\n",
    "threat_data = data[(data.threat == 1)]\n",
    "insult_data = data[(data.insult == 1)]\n",
    "insult_data = data[(data.identity_hate == 1)]\n",
    "\n",
    "# Make some grams to inspect the data\n",
    "insult_unigram, insult_bigram = make_grams(insult_data)\n",
    "threat_unigram, threat_bigram = make_grams(threat_data)\n",
    "obscene_unigram, obscene_bigram = make_grams(obscene_data)\n",
    "toxic_unigram, toxic_bigram = make_grams(toxic_data)\n",
    "severe_toxic_unigram, severe_toxic_bigram = make_grams(severe_toxic_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_comments(data, toxic = 0, severe_toxic = 0, obscene = 0, threat = 0, insult = 0, identity_hate = 0):\n",
    "    condition = (data.toxic == toxic) & (data.severe_toxic == severe_toxic) & (data.obscene == obscene) & (data.threat == threat) & (data.insult == insult) & (data.identity_hate == identity_hate)\n",
    "    return data[condition]\n",
    "    \n",
    "    \n",
    "\n",
    "toxic_only = filter_comments(data, toxic = 1)\n",
    "severe_toxic_only = filter_comments(data, severe_toxic = 1)\n",
    "obscene_only = filter_comments(data, obscene = 1)\n",
    "threat_only = filter_comments(data, threat = 1)\n",
    "insult_only = filter_comments(data, insult = 1)\n",
    "identity_hate_only = filter_comments(data, identity_hate = 1)\n",
    "cleant_documents = filter_comments(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of documents that belong to only ONE class\n",
      "============\n",
      "Total documents:  153164\n",
      "Clean documents:  0.377  , count:  57735\n",
      "============\n",
      "Only toxic documents:  0.011  , count:  1710\n",
      "Only severe toxic documents:  0.0  , count:  0\n",
      "Only obscene documents:  0.00032  , count:  49\n",
      "Only threath documents:  3.3e-05  , count:  5\n",
      "Only insult documents:  0.00042  , count:  64\n",
      "Only identity hate documents:  9e-05  , count:  14\n"
     ]
    }
   ],
   "source": [
    "total_documents = len(data)\n",
    "\n",
    "clean_document_count = len(cleant_documents)\n",
    "toxic_document_count = len(toxic_only)\n",
    "severe_toxic_document_count = len(severe_toxic_only)\n",
    "obscene_document_count = len(obscene_only)\n",
    "threat_document_count = len(threat_only)\n",
    "insult_document_count = len(insult_only)\n",
    "identity_hate_document_count = len(identity_hate_only)\n",
    "\n",
    "print(\"Precentage of documents that belong to only ONE class\")\n",
    "print(\"============\")\n",
    "print(\"Total documents: \", total_documents)\n",
    "print(\"Clean documents: \", round(clean_document_count / total_documents, 3),  \" , count: \", clean_document_count)\n",
    "print(\"============\")\n",
    "print(\"Only toxic documents: \", round(toxic_document_count / total_documents, 3),  \" , count: \", toxic_document_count)\n",
    "print(\"Only severe toxic documents: \", round(severe_toxic_document_count / total_documents, 3),  \" , count: \", severe_toxic_document_count)\n",
    "print(\"Only obscene documents: \", round(obscene_document_count / total_documents, 5),  \" , count: \", obscene_document_count)\n",
    "print(\"Only threath documents: \", round(threat_document_count / total_documents, 6),  \" , count: \", threat_document_count)\n",
    "print(\"Only insult documents: \", round(insult_document_count / total_documents, 5),  \" , count: \", insult_document_count)\n",
    "print(\"Only identity hate documents: \", round(identity_hate_document_count / total_documents, 5),  \" , count: \", identity_hate_document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('boob', 'boob'), 999),\n",
       " (('poop', 'poop'), 954),\n",
       " (('faggot', 'faggot'), 647),\n",
       " (('i', 'hate'), 496),\n",
       " ((\"''\", \"''\"), 464),\n",
       " (('hate', 'you'), 464),\n",
       " (('die', 'die'), 455),\n",
       " (('you', 'i'), 453),\n",
       " (('``', \"''\"), 402),\n",
       " (('hate', 'hate'), 367),\n",
       " (('analanal', 'anal'), 350),\n",
       " (('anal', 'analanal'), 350),\n",
       " (('bums', 'bums'), 349),\n",
       " (('kill', 'yourself'), 247),\n",
       " (('yourself', 'kill'), 246),\n",
       " (('balls', 'balls'), 217),\n",
       " (('balls', 'ballsballs'), 215),\n",
       " (('ballsballs', 'balls'), 215),\n",
       " (('...', '...'), 213),\n",
       " (('anime', 'rules'), 185)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_unigram, insult_bigram = make_grams(toxic_only)\n",
    "insult_bigram.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "\n",
    "X = train.iloc[:, 0]\n",
    "Y = train.iloc[:, 1:]\n",
    "\n",
    "corpus = Corpus(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "addmm(): argument 'mat1' (position 1) must be Variable, not torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-efbf8695b18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-185-efbf8695b18f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, bow_vec)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoWClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: addmm(): argument 'mat1' (position 1) must be Variable, not torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# Make a BOW with unique integers!\n",
    "\"\"\"\n",
    "word_to_ix = {}\n",
    "for sent in X:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 31\n",
    "\n",
    "# Assign a integer to a word\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "# Calculate the target\n",
    "def make_target(label):\n",
    "    tmp = 0\n",
    "    for test in Y.iloc[49]:\n",
    "        tmp += test\n",
    "    return torch.LongTensor([0])\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for index in range(len(X)):\n",
    "    model.zero_grad()\n",
    "    bow_vec = make_bow_vector(X.iloc[index], word_to_ix)\n",
    "    target = make_target(Y.iloc[index])\n",
    "    \n",
    "    log_probs = model(bow_vec)\n",
    "\"\"\"\n",
    "# TO FIX!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
