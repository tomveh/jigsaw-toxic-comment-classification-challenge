{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# KINDA EXPERIMENTAL - USED FOR EXPLORATION OF DATA - dont expect this to run as it is\n",
    "# ===========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, RegexpStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer() \n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stop.append(\"!\")\n",
    "stop.append(',')\n",
    "stop.append('')\n",
    "stop.append('=')\n",
    "stop = list(stop)\n",
    "re = RegexpStemmer('[0-9]+')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(string):\n",
    "    # Annoying things!\n",
    "    string = string.replace(\"=\", \"\")\n",
    "    string = string.replace(\"-\", \"\")\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    tokens = word_tokenize(str(string))\n",
    "    # Punctuations\n",
    "    tokens = [re.stem(token.lower()) for token in tokens if token not in punctuation and token not in stop]\n",
    "    # Stopwords\n",
    "    #tokens = [token for token in tokens if token not in stop]\n",
    "    # Numbers\n",
    "    #tokens = [re.stem(token) for token in tokens]\n",
    "    # convert to lowercase\n",
    "    #tokens = [token.lower() for token in tokens]\n",
    "    return (tokens)\n",
    "\n",
    "# Make unigrams and bygrams from a classified data \n",
    "def make_grams(classified_data):\n",
    "    unigram_frequencies = Counter([])\n",
    "    #bigram_frequencies = Counter([])\n",
    "    \n",
    "    sentances = classified_data[\"comment_text\"]\n",
    "    for sentance in sentances:\n",
    "        unigrams = ngrams(sentance, 1)\n",
    "        # bigrams = ngrams(sentance, 2)\n",
    "        unigram_frequencies += Counter(unigrams)\n",
    "        # bigram_frequencies += Counter(bigrams)\n",
    "    return unigram_frequencies# , bigram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('test.csv', index_col=0)\n",
    "#train_labels = pd.read_csv('test_labels.csv', index_col=0)\n",
    "\n",
    "# Process data!\n",
    "#train[\"comment_text\"] = train.comment_text.apply(preprocess_text)\n",
    "#data = pd.concat([train, train_labels], axis=1)\n",
    "\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "data[\"comment_text\"] = data.comment_text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that gets documents that only belong to only one of the classes\n",
    "def filter_comments(data, toxic = 0, severe_toxic = 0, obscene = 0, threat = 0, insult = 0, identity_hate = 0):\n",
    "    condition = (data.toxic == toxic) & (data.severe_toxic == severe_toxic) & (data.obscene == obscene) & (data.threat == threat) & (data.insult == insult) & (data.identity_hate == identity_hate)\n",
    "    return data[condition]\n",
    "    \n",
    "    \n",
    "toxic_only = filter_comments(data, toxic = 1)\n",
    "severe_toxic_only = filter_comments(data, severe_toxic = 1)\n",
    "obscene_only = filter_comments(data, obscene = 1)\n",
    "threat_only = filter_comments(data, threat = 1)\n",
    "insult_only = filter_comments(data, insult = 1)\n",
    "identity_hate_only = filter_comments(data, identity_hate = 1)\n",
    "cleant_documents = filter_comments(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of documents that belong to ONLY ONE class\n",
      "============\n",
      "Total documents:  159571\n",
      "Clean documents: \t\t0.898321%\t ,count: 143346\n",
      "============\n",
      "Only toxic documents: \t\t0.035508%\t ,count: 5666\n",
      "Only severe toxic documents: \t0.0%\t ,count: 0\n",
      "Only obscene documents: \t0.001987%\t ,count: 317\n",
      "Only threath documents: \t0.000138%\t ,count: 22\n",
      "Only insult documents: \t\t0.001886%\t ,count: 301\n",
      "Only identity documents: \t0.000338%\t ,count: 54\n"
     ]
    }
   ],
   "source": [
    "# Method to print precenage of documents compared to larger subset\n",
    "def partition(message, all_docs, subset_docs):\n",
    "    return message + \"\\t\" +  str(round(len(subset_docs) / len(all_docs), 6)) + \"%\\t ,count: \" + str(len(subset_docs))\n",
    "\n",
    "\n",
    "total_documents = len(data)\n",
    "clean_document_count = len(cleant_documents)\n",
    "toxic_document_count = len(toxic_only)\n",
    "severe_toxic_document_count = len(severe_toxic_only)\n",
    "obscene_document_count = len(obscene_only)\n",
    "threat_document_count = len(threat_only)\n",
    "insult_document_count = len(insult_only)\n",
    "identity_hate_document_count = len(identity_hate_only)\n",
    "\n",
    "print(\"Precentage of documents that belong to ONLY ONE class\")\n",
    "print(\"============\")\n",
    "print(\"Total documents: \", total_documents)\n",
    "print(partition(\"Clean documents: \\t\", data, cleant_documents))\n",
    "print(\"============\")\n",
    "print(partition(\"Only toxic documents: \\t\", data, toxic_only))\n",
    "print(partition(\"Only severe toxic documents: \", data, severe_toxic_only))\n",
    "print(partition(\"Only obscene documents: \", data, obscene_only))\n",
    "print(partition(\"Only threath documents: \", data, threat_only))\n",
    "print(partition(\"Only insult documents: \\t\", data, insult_only))\n",
    "print(partition(\"Only identity documents: \", data, identity_hate_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of documents that belong to a class\n",
      "============\n",
      "Total documents:  159571\n",
      "Clean documents: \t\t0.898321%\t ,count: 143346\n",
      "============\n",
      "Toxic documents: \t\t0.095844%\t ,count: 15294\n",
      "Severe toxic documents: \t0.009996%\t ,count: 1595\n",
      "Obscene documents \t: \t0.052948%\t ,count: 8449\n",
      "Threat documents: \t\t0.002996%\t ,count: 478\n",
      "Insult documents: \t\t0.049364%\t ,count: 7877\n",
      "Identity hate documents: \t0.008805%\t ,count: 1405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10167887648758234"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_toxic = data[(data.toxic == 1)]\n",
    "has_severe_toxic = data[(data.severe_toxic == 1)]\n",
    "has_obscene = data[(data.obscene == 1)]\n",
    "has_threat = data[(data.threat == 1)]\n",
    "has_insult = data[(data.insult == 1)]\n",
    "has_identity_hate = data[(data.identity_hate == 1)]\n",
    "\n",
    "\n",
    "print(\"Precentage of documents that belong to a class\")\n",
    "print(\"============\")\n",
    "print(\"Total documents: \", total_documents)\n",
    "print(partition(\"Clean documents: \\t\", data, cleant_documents))\n",
    "print(\"============\")\n",
    "print(partition(\"Toxic documents: \\t\", data, has_toxic))\n",
    "print(partition(\"Severe toxic documents: \", data, has_severe_toxic))\n",
    "print(partition(\"Obscene documents \\t: \", data, has_obscene))\n",
    "print(partition(\"Threat documents: \\t\", data, has_threat))\n",
    "print(partition(\"Insult documents: \\t\", data, has_insult))\n",
    "print(partition(\"Identity hate documents: \", data, has_identity_hate))\n",
    "\n",
    "( total_documents - clean_document_count ) / total_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_comments(data, condition):\n",
    "    return data[condition]\n",
    "\n",
    "classes = {}\n",
    "classes[\"Toxic\"] = (data.toxic == 1)\n",
    "classes[\"SevereToxic\"] = (data.severe_toxic == 1)\n",
    "classes[\"Obscene\"] = (data.obscene == 1)\n",
    "classes[\"Threat\"] = (data.threat == 1)\n",
    "classes[\"Insult\"] = (data.insult == 1)\n",
    "classes[\"IdentityHate\"] = (data.identity_hate == 1)\n",
    "\n",
    "class_library = [\"Toxic\", \"SevereToxic\", \"Obscene\", \"Threat\", \"Insult\", \"IdentityHate\"]\n",
    "freq_matrix = np.zeros((len(classes), len(classes)))\n",
    "conditional_appearence = np.zeros((len(classes), len(classes)))\n",
    "co_occurance = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "# How frequent classes are together?\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for class1 in class_library:\n",
    "    counter2 = 0\n",
    "    for class2 in class_library:\n",
    "        condition = classes[class1] & classes[class2]\n",
    "        common_elements = filter_comments(data, condition)\n",
    "        freq_matrix[counter1,counter2] =  round(len(common_elements)  / (len(data) - len(cleant_documents)), 3)\n",
    "        counter2 += 1\n",
    "    counter1+= 1\n",
    "        \n",
    "# How many of the classes co-occur if one occurs?\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for class1 in class_library:\n",
    "    counter2 = 0\n",
    "    class_elements = filter_comments(data, classes[class1])\n",
    "    for class2 in class_library:\n",
    "        condition2 = classes[class1] & classes[class2]\n",
    "        common_elements = filter_comments(data, condition2)\n",
    "        conditional_appearence[counter1,counter2] =  round(len(common_elements)  / (len(class_elements)), 3)\n",
    "        counter2 += 1\n",
    "    counter1+= 1\n",
    "\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for class1 in class_library:\n",
    "    counter2 = 0\n",
    "    class1_elements = filter_comments(data, classes[class1])\n",
    "    for class2 in class_library:\n",
    "        class2_elements = filter_comments(data, classes[class2])\n",
    "        condition2 = classes[class1] & classes[class2]\n",
    "        common_elements = filter_comments(data, condition2)\n",
    "        co_occurance[counter1,counter2] =  round(len(common_elements)  / (len(class1_elements) + len(class2_elements)), 3)\n",
    "        counter2 += 1\n",
    "    counter1+= 1\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(freq_matrix)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(conditional_appearence)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(co_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many words comments in certain class have\n",
    "def calculate_avarage_words(dataframe):\n",
    "    words = 0\n",
    "    for comment in dataframe[\"comment_text\"]:\n",
    "        words += len(comment)\n",
    "    \n",
    "    return round(words / len(dataframe))\n",
    "\n",
    "\n",
    "print(\"Avarage ammount words per comment\")\n",
    "print(\"=======\")\n",
    "print(\"Clean docs: \\t\", calculate_avarage_words(cleant_documents))\n",
    "print(\"=======\")\n",
    "print(\"Toxic docs: \\t\", calculate_avarage_words(has_toxic))\n",
    "print(\"Severe Toxic: \\t\", calculate_avarage_words(has_severe_toxic))\n",
    "print(\"Obscene docs: \\t\", calculate_avarage_words(has_obscene))\n",
    "print(\"Threat docs: \\t\", calculate_avarage_words(has_threat))\n",
    "print(\"Insult docs: \\t\", calculate_avarage_words(has_insult))\n",
    "print(\"Hate docs: \\t\", calculate_avarage_words(has_identity_hate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insult_unigram = make_grams(cleant_documents)\n",
    "\n",
    "insult_unigram.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NN with BOW or something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CUDA devices:  1\n",
      "Quadro K2200\n",
      "<torch.cuda.device object at 0x7f081c2cea20>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print('number of CUDA devices: ',torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.cuda.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "\n",
    "X = train.comment_text.apply(preprocess_text)\n",
    "Y = train.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make naive BOW-presentation\n",
    "word_to_ix = {}\n",
    "for sent in X:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = np.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec\n",
    "\n",
    "def make_target(label):\n",
    "    tmp = []\n",
    "    for lab in label:\n",
    "        tmp.append(lab)\n",
    "    return tuple( np.array(tmp) )\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, n_hidden1, n_hidden2):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.hidden1 = nn.Linear(vocab_size, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.out = nn.Linear(n_hidden2, num_labels)\n",
    "\n",
    "    def forward(self, x_val):\n",
    "        x = Variable(x_val, requires_grad=False)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x\n",
    "    \n",
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_labels, vocab_size, hidden_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_size)\n",
    "        self.hidden2label = nn.Linear(hidden_size, num_labels)\n",
    "        self.hidden = (Variable(torch.zeros(1, 1, hidden_size)),\n",
    "                        Variable(torch.zeros(1, 1, hidden_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(x),1, -1), self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        probs = F.sigmoid(y)\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG was 0.653\n",
      "AVG was 0.641\n",
      "AVG was 0.612\n",
      "AVG was 0.598\n",
      "AVG was 0.584\n",
      "AVG was 0.567\n",
      "AVG was 0.543\n",
      "AVG was 0.524\n",
      "AVG was 0.493\n",
      "AVG was 0.455\n",
      "AVG was 0.418\n",
      "AVG was 0.370\n",
      "AVG was 0.332\n",
      "AVG was 0.294\n",
      "AVG was 0.264\n",
      "AVG was 0.237\n",
      "AVG was 0.209\n",
      "AVG was 0.189\n",
      "AVG was 0.172\n",
      "AVG was 0.157\n",
      "AVG was 0.144\n",
      "AVG was 0.132\n",
      "AVG was 0.122\n",
      "AVG was 0.111\n",
      "AVG was 0.102\n",
      "AVG was 0.095\n",
      "AVG was 0.089\n",
      "AVG was 0.083\n",
      "AVG was 0.078\n",
      "AVG was 0.073\n",
      "AVG was 0.069\n",
      "AVG was 0.065\n",
      "AVG was 0.062\n",
      "AVG was 0.059\n",
      "AVG was 0.056\n",
      "AVG was 0.054\n",
      "AVG was 0.051\n",
      "AVG was 0.049\n",
      "AVG was 0.047\n",
      "AVG was 0.045\n",
      "AVG was 0.044\n",
      "AVG was 0.042\n",
      "AVG was 0.041\n",
      "AVG was 0.039\n",
      "AVG was 0.038\n",
      "AVG was 0.036\n",
      "AVG was 0.035\n",
      "AVG was 0.034\n",
      "AVG was 0.033\n",
      "AVG was 0.032\n",
      "AVG was 0.031\n",
      "AVG was 0.030\n",
      "AVG was 0.029\n",
      "AVG was 0.028\n",
      "AVG was 0.027\n",
      "AVG was 0.027\n",
      "AVG was 0.026\n",
      "AVG was 0.025\n",
      "AVG was 0.025\n",
      "AVG was 0.024\n",
      "AVG was 0.023\n",
      "AVG was 0.023\n",
      "AVG was 0.022\n",
      "AVG was 0.022\n",
      "AVG was 0.021\n",
      "AVG was 0.021\n",
      "AVG was 0.020\n",
      "AVG was 0.020\n",
      "AVG was 0.019\n",
      "AVG was 0.019\n",
      "AVG was 0.018\n",
      "AVG was 0.018\n",
      "AVG was 0.018\n",
      "AVG was 0.017\n",
      "AVG was 0.017\n",
      "AVG was 0.016\n",
      "AVG was 0.016\n",
      "AVG was 0.016\n",
      "AVG was 0.015\n",
      "AVG was 0.015\n",
      "AVG was 0.015\n",
      "AVG was 0.015\n",
      "AVG was 0.014\n",
      "AVG was 0.014\n",
      "AVG was 0.014\n",
      "AVG was 0.013\n",
      "AVG was 0.013\n",
      "AVG was 0.013\n",
      "AVG was 0.013\n",
      "AVG was 0.012\n",
      "AVG was 0.012\n",
      "AVG was 0.012\n",
      "AVG was 0.012\n",
      "AVG was 0.012\n",
      "AVG was 0.011\n",
      "AVG was 0.011\n",
      "AVG was 0.011\n",
      "AVG was 0.011\n",
      "AVG was 0.011\n",
      "AVG was 0.010\n",
      "AVG was 0.010\n",
      "AVG was 0.010\n",
      "AVG was 0.010\n",
      "AVG was 0.010\n",
      "AVG was 0.010\n",
      "AVG was 0.010\n",
      "AVG was 0.009\n",
      "AVG was 0.009\n",
      "AVG was 0.009\n",
      "AVG was 0.009\n",
      "AVG was 0.009\n",
      "AVG was 0.009\n",
      "AVG was 0.009\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.008\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.007\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.006\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.005\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.004\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.003\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.002\n",
      "AVG was 0.001\n",
      "AVG was 0.001\n",
      "AVG was 0.001\n",
      "AVG was 0.001\n",
      "AVG was 0.001\n",
      "AVG was 0.001\n",
      "AVG was 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-53a030587911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 100 # len(word_to_ix) # 100\n",
    "HIDDEN1 = 8\n",
    "NUM_LABELS = 6\n",
    "\n",
    "model = LSTMClassifier(1, NUM_LABELS, VOCAB_SIZE, HIDDEN1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 1\n",
    "num_batches = int(len(X) * 0.9) // batch_size\n",
    "critize = nn.BCELoss()\n",
    "\n",
    "X_train = X[:int(len(X) * 0.95)]\n",
    "y_train = Y[:int(len(X) * 0.95)]\n",
    "\n",
    "X_test = X[int(len(X) * 0.95) + 1 :]\n",
    "y_test = Y[int(len(X) * 0.95) + 1 :]\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(10):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        test_x = np.array([d2v_model.infer_vector(comment) for comment in X_batch ])\n",
    "        test_y = np.array([make_target(Y.iloc[i]) for i in range(len(y_batch)) ])\n",
    "\n",
    "        #test_x = np.array([make_bow_vector(X[0], word_to_ix)])\n",
    "        #test_y = np.array([make_target(Y.iloc[0])])\n",
    "\n",
    "        test_x = Variable(torch.from_numpy(test_x))\n",
    "        test_y = Variable(torch.from_numpy(test_y).float())\n",
    "\n",
    "        output = model.forward(test_x)\n",
    "        loss = critize(output, test_y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "\n",
    "    print(\"AVG was %.3f\" % np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "HIDDEN1 = 32\n",
    "HIDDEN2 = 16\n",
    "NUM_LABELS = 6\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE, HIDDEN1, HIDDEN2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# MultiLabelSoftMarginLoss - \n",
    "epochs = 15\n",
    "batch_size = 1000\n",
    "num_batches = int(len(X) * 0.9) // batch_size\n",
    "# critize = nn.MultiLabelSoftMarginLoss()\n",
    "critize = nn.BCELoss()\n",
    "\n",
    "\n",
    "X_train = X[:int(len(X) * 0.95)]\n",
    "y_train = Y[:int(len(X) * 0.95)]\n",
    "\n",
    "X_test = X[int(len(X) * 0.95) + 1 :]\n",
    "y_test = Y[int(len(X) * 0.95) + 1 :]\n",
    "\n",
    "loss_table = []\n",
    "score_table = []\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(num_batches):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        # Fetch the part of data neededVariable\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        # Prepare data to BOW and make torch vectors\n",
    "        X_batch = np.array([make_bow_vector(comment, word_to_ix) for comment in X_batch])\n",
    "        y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).float(), requires_grad=False)\n",
    "        # GD towards opt\n",
    "        model.zero_grad()\n",
    "        output_fw = model.forward(X_batch)\n",
    "\n",
    "        loss = critize(output_fw, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "\n",
    "    X_batch = X_test\n",
    "    y_batch = y_test\n",
    "    X_batch = np.array([make_bow_vector(comment, word_to_ix) for comment in X_batch])\n",
    "    y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "    X_batch = torch.from_numpy(X_batch).float()\n",
    "    # GD towards opt\n",
    "    model.zero_grad()\n",
    "    output_fw = model.forward(X_batch)\n",
    "    score = roc_auc_score(y_batch, output_fw.data.numpy())\n",
    "\n",
    "    print('[%d/%d] Training-error: %.3f' % (epoch+1, epochs, np.mean(losses)))\n",
    "    print('[%d/%d] ROC-score:  %.3f' % (epoch+1, epochs, score))\n",
    "    \n",
    "    loss_table.append(np.mean(losses))\n",
    "    score_table.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user gensim\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='ROC AUC score')\n",
    "red_patch = mpatches.Patch(color='red', label='Mean Binary cross entropy loss')\n",
    "\n",
    "plt.plot(range(1,21), score_table)\n",
    "plt.plot(range(1,21), loss_table, color='red')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_comment_word2vec(comment, word_vectors):\n",
    "    doc_vector = [np.sum(word_vectors[word]) for word in sentance if word in word_vectors.vocab]\n",
    "    doc_vector = np.sum(doc_vector, axis=0)\n",
    "    return doc_vector\n",
    "\n",
    "# Word2Vec model\n",
    "# Load google's w2v\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=350000) \n",
    "word_vectors = word2vec.wv\n",
    "\n",
    "sentance = X[0]\n",
    "word_mean = mean_of_comment_word2vec(sentance, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100\n",
    "HIDDEN1 = 256\n",
    "HIDDEN2 = 128\n",
    "NUM_LABELS = 6\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE, HIDDEN1, HIDDEN2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 1000\n",
    "num_batches = int(len(X) * 0.8) // batch_size\n",
    "#critize = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "critize = nn.BCELoss()\n",
    "\n",
    "\n",
    "X_train = X[:int(len(X) * 0.8)]\n",
    "y_train = Y[:int(len(X) * 0.8)]\n",
    "\n",
    "X_test = X[int(len(X) * 0.8) + 1 :]\n",
    "y_test = Y[int(len(X) * 0.8) + 1 :]\n",
    "\n",
    "loss_table = []\n",
    "score_table = []\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(num_batches):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        # Fetch the part of data needed\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        # Prepare data to BOW and make torch vectors\n",
    "        X_batch = np.array([d2v_model.infer_vector(comment) for comment in X_batch])\n",
    "        y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "        \n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).float(), requires_grad=False)\n",
    "        # GD towards opt\n",
    "        model.zero_grad()\n",
    "        output_fw = model.forward(X_batch)\n",
    "\n",
    "        loss = critize(output_fw, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "        \n",
    "    X_batch = X_test\n",
    "    y_batch = y_test\n",
    "    X_batch = np.array([d2v_model.infer_vector(comment) for comment in X_batch])\n",
    "    y_batch = np.array([ make_target(y_batch.iloc[i]) for i in range(len(y_batch))])\n",
    "    X_batch = torch.from_numpy(X_batch).float()\n",
    "    # GD towards opt\n",
    "    model.zero_grad()\n",
    "    output_fw = model.forward(X_batch)\n",
    "    score = roc_auc_score(y_batch, output_fw.data.numpy())\n",
    "\n",
    "    print('[%d/%d] Training-error: %.3f' % (epoch+1, epochs, np.mean(losses)))\n",
    "    print('[%d/%d] ROC-score:  %.3f' % (epoch+1, epochs, score))\n",
    "    \n",
    "    loss_table.append(np.mean(losses))\n",
    "    score_table.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 54s, sys: 32.4 s, total: 14min 26s\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "def read_corpus(X):\n",
    "    for i, line in enumerate(X):\n",
    "        line = \" \".join(line)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=55)\n",
    "\n",
    "corpus = list(read_corpus(X))\n",
    "d2v_model.build_vocab(corpus)\n",
    "%time d2v_model.train(corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "SVM_pl = Pipeline([\n",
    "    #('vectorizer', CountVectorizer(binary=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores_SVC = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X[train_idx], Y.iloc[train_idx, :].values\n",
    "    X_test, y_test = X[test_idx], Y.iloc[test_idx, :].values\n",
    "    X_train = np.array([d2v_model.infer_vector(comment) for comment in X_train])\n",
    "    X_test = np.array([d2v_model.infer_vector(comment) for comment in X_test])\n",
    "    \n",
    "    SVM_pl.fit(X_train, y_train)\n",
    "    y_pred = SVM_pl.predict(X_test)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    scores_SVC.append(score)\n",
    "    print(score)\n",
    "    \n",
    "print('MEAN: {}'.format(np.mean(scores_SVC)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.array([d2v_model.infer_vector(X[6])])\n",
    "test_x = Variable(torch.from_numpy(test_x))\n",
    "\n",
    "model.forward(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do you know he is dead.  Its just his plane that crashed.  Jeezz, quit busting his nuts, folks.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_obscene.iloc[29][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
